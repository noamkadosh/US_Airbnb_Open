---
title: "US Airbnb Open Data Project Report"
author: "Noam Kadosh"
date: "December 17th, 2020"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
---

\pagebreak

# Overview
This project is part of the HarvardX course PH125.9x Data Science: Capstone project.
When Airbnb was founded in 2008, the idea of sleeping in other people's house was somewhat strange. Despite that, Airbnb grew larger each year with increasing number of listings and users each year. There was a real advantage to that idea which was the real catalyst for the company's growth. People understood it is cheaper than the classic approach.

## Project Introduction
In this project I will explore the US Airbnb Open dataset and try to predict the prices of Airbnb listings, both new listings and existing listings. The model can fit a property management company who wants to know the right price of a listings. With the model, a company can find undervalued listings and try to offer them services or purchase the listing.
The price of listing is a continuous variable. Therefore, regression algorithms are the go to tool. I will train several regression algorithms and one ensemble algorithm. Than, I will measure their performance by the Root Mean Squared Error (RMSE). The goal is to get the RMSE as low as possible.

### Libraries
```{r libraries, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Loading libraries
repos = "http://cran.us.r-project.org"
if(!require(tidyverse)) install.packages("tidyverse", repos = repos)
if(!require(tidytext)) install.packages("tidytext", repos = repos)
if(!require(caret)) install.packages("caret", repos = repos)
if(!require(stringr)) install.packages("stringr", repos = repos)
if(!require(scales)) install.packages("scales", repos = repos)
if(!require(lubridate)) install.packages("lubridate", repos = repos)
if(!require(rpart)) install.packages("rpart", repos = repos)
if(!require(rpart.plot)) install.packages("rpart.plot", repos = repos)
if(!require(randomForest)) install.packages("randomForest", repos = repos)
if(!require(kernlab)) install.packages("kernlab", repos = repos)
if(!require(fastDummies)) install.packages("fastDummies", repos = repos)
if(!require(lubridate)) install.packages("lubridate", repos = repos)
if(!require(corrplot)) install.packages("corrplot", repos = repos)
if(!require(wordcloud)) install.packages("wordcloud", repos = repos)
```

## The Data
The data used for the project is the US Airbnb Open data set, collected by Kritik Seth, a data scientist who collected and published it on Kaggle. The data set contains features such as location, room type, description, number of reviews, minimum nights and more.
```{r load_data, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Loading the data set
airbnb <- read_csv("Data/AB_US_2020.csv", col_types = cols(
  neighbourhood = col_character(),
  neighbourhood_group = col_character()
))

head(airbnb)
summary(airbnb)
length(airbnb$price)
airbnb %>% summarize(num_properties = n_distinct(id), num_hosts = n_distinct(host_id))
```

### Data Preparation
We can see that some of the columns contain NAs. We should try and understand why those NAs are there. Looking at the neighbourhood_group column we can infer that those NAs are other neighbourhood_group that are unknown to us. We can transform those NAs to "other".
When looking at the reviews_per_month and the number_of_reviews columns we can see that the NAs in the reviews_per_month are actually 0. All those NAs have 0 reviews overall.
The last_review column will be converted to numeric representing days since the epoch (January 1st 1970). Listings with NA value will get the value 0, meaning no review was ever recorded.
We will also filter out listings with minimum nights of more than 365.
To handle the rest of the NAs in the data, I decided to omit them since they are a very small subset of the data (just about 60 rows).
Finally, I will add the State column based on the City column.
```{r data-preparation, echo = TRUE, message = FALSE, warning = FALSE, eval = TRUE}
# Changing NAs in neighbourhood_group column to "other".
airbnb$neighbourhood_group[is.na(airbnb$neighbourhood_group)] <- "other"

# Changing NAs in reviews_per_month column to 0.
airbnb$reviews_per_month[is.na(airbnb$reviews_per_month)] <- 0

# Converting last_review column to numeric
airbnb <- airbnb %>%
  mutate(last_review = as.numeric(as.Date(airbnb$last_review, "%d/%m/%y")))
airbnb$last_review[is.na(airbnb$last_review)] <- 0

# Filtering listings with more than a year stay minimum.
airbnb <- airbnb %>%
  filter(minimum_nights < 365)

# Removing all NAs
airbnb <- na.omit(airbnb)

# Adding the State column
temp <- tibble(city = c("Asheville", "Austin", "Boston", "Broward County", "Cambridge",
                        "Chicago", "Clark County", "Columbus", "Denver", "Hawaii",
                        "Jersey City", "Los Angeles", "Nashville", "New Orleans",
                        "New York City", "Oakland", "Pacific Grove", "Portland",
                        "Rhode Island", "Salem", "San Clara Country",
                        "Santa Cruz County", "San Diego", "San Francisco",
                        "San Mateo County", "Seattle", "Twin Cities MSA",
                        "Washington D.C."),
               state = c("NC", "TX", "MA", "FL", "MA", "IL", "NV", "OH", "CO", "HI",
                         "NJ", "CA", "TN", "LA", "NY", "CA", "CA", "OR", "RI", "MA",
                         "CA", "CA", "CA", "CA", "CA", "WA", "MN", "DC"))
airbnb_states <- sapply(airbnb$city, function(x) {
  temp$state[which(temp$city == x)]
})
airbnb <- airbnb %>%
  mutate(state = airbnb_states)
```

A summary of the data after preparation.
```{r data-exploration, echo = TRUE}
head(airbnb)
summary(airbnb)
length(airbnb$price)
airbnb %>% summarize(num_properties = n_distinct(id), num_hosts = n_distinct(host_id))
```

### Data Analysis
When observing the price distribution on log scale, we can see the majority of the data is around $100.
```{r price-distribution-with-outliers, echo = TRUE}
# Price Distribution
airbnb %>%
  ggplot(aes(price)) +
  geom_histogram(binwidth = 0.25, color = "black", fill = "#2e4057") +
  scale_x_continuous(name = "Price",trans = "log", breaks = c(1, 10, 100, 1000, 10000)) +
  ylab("Count") +
  ggtitle("Price Distribution")
```

Because of this, we can safely remove extreme outliers, as they will probably hurt our models (more on that later). Note that we only lose about 5000 rows.
```{r price-distribution, echo = TRUE}
# Get rid of extreme outliers. 
airbnb <- airbnb %>%
  filter(price > 10 & price < 1000)
# Price Distribution
airbnb %>%
  ggplot(aes(price)) +
  geom_histogram(binwidth = 0.25, color = "black", fill = "#2e4057") +
  scale_x_continuous(name = "Price",trans = "log", breaks = c(10, 100, 1000)) +
  ylab("Count") +
  ggtitle("Price Distribution")  
```

Now let's have a look at our features.
The room_type column is a categorical feature with four values: entire home, private room, shared room, and hotel room. Airbnb's niche is about renting from other people, which explains the fact that hotel rooms are so rare in the data. A shared room, hostel style, is also very rare in the dataset. Looking at the average price by room type, we can see what we would expect as hotel rooms are most expensive, followed closely by entire homes, and than private rooms and shared rooms with cheaper prices.
```{r room-type-distribution, echo = TRUE}
# Room Type Distribution
airbnb$room_type <- fct_infreq(airbnb$room_type)
airbnb %>%
  ggplot(aes(room_type, label = percent(prop.table(stat(count))))) +
  geom_bar(color = "black", fill = "#2e4057") +
  geom_text(stat = "count", position = position_dodge(width = .9), vjust = -0.5,
            size = 3) +
  xlab("Room Type") +
  scale_y_continuous(name = "Count", trans = "log", labels = comma, 
                     breaks = c(10, 1000, 1000, 10000, 100000)) +
  ggtitle("Room Type Distribution")

# Average Price by Room Type
airbnb %>%
  group_by(room_type) %>%
  summarize(mean = mean(price), .groups = "drop") %>%
  ggplot(aes(x = reorder(room_type, mean), mean, 
             label = dollar(round(mean, digits = 2)))) +
  geom_col(color = "black", fill = "#2e4057") +
  geom_text(position = position_dodge(width = .9), vjust = -0.5, size = 3) +
  xlab("Room Type") +
  scale_y_continuous(name = "Count", labels = comma) +
  ggtitle("Average Price by Room Type")
```

As we saw earlier, there is a connection between number of reviews and reviews per month columns. Their distributions also look very similar, with the majority of the data having 0 reviews (or reviews per month) and decreasing number of reviews as we go up the scale. These two columns together hold another important property which is the age of the listing. We can get that by the follwing formula: 
$$ (NumberOfReviews / ReviewsPerMonth) / 12 = Age  $$
Unfortunately, rows with 0 are missing that information. Anyway, listings with 0 reviews can be either inactive, new, or just poorly managed (e.g. a 5 year old listing with 0 reviews).
```{r reviews-distribution, echo = TRUE}
# Number of Review Distribution
airbnb %>%
 ggplot(aes(number_of_reviews)) +
 geom_histogram(binwidth = 0.2, color = "black", fill = "#2e4057") +
 scale_x_continuous(name = "Number of Reviews",trans = pseudo_log_trans(base = 10), 
                    breaks = c(1, 10, 100, 1000)) +
 ylab("Count") +
 ggtitle("Number of Reviews Distribution") 

# Reviews Per Month Distribution
airbnb %>%
  ggplot(aes(reviews_per_month)) +
  geom_histogram(binwidth = 0.2, color = "black", fill = "#2e4057") +
  scale_x_continuous(name = "Reviews Per Month",trans = pseudo_log_trans(base = 10), 
                     breaks = c(1, 10)) +
  ylab("Count") +
  ggtitle("Reviews Per Month Distribution")
```

The last review distribution shows us that most of the listings were active recently, and very few got their last review years ago.
```{r last-review-distribution, echo = TRUE}
# Last Review Distribution
airbnb %>%
  ggplot(aes(last_review)) +
  geom_bar(stat = "count", width = 0.9, color = "black", fill = "#2e4057") +
  xlab("Days") +
  ylab("Count") +
  ggtitle("Last Review Distribution")

# Last Review Distribution - without 0's.
airbnb %>%
  filter(last_review > 0) %>%
  ggplot(aes(last_review)) +
  geom_bar(stat = "count", width = 0.9, color = "black", fill = "#2e4057") +
  xlab("Day") +
  ylab("Count") +
  ggtitle("Last Review Distribution")
```

Most of the listings in the dataset either have 0 availability or 365 (full year) availability. We can assume those with 0 availability are successful listings while those with full availability are either inactive or new. Some of the listings can be affected by seasonality, which explains why they are available for most of the year. The average listing availability by state plot shows us that Hawaii and Florida average the most availability. We know these state are highly touristic in the summer.
```{r availability-365--distribution, echo = TRUE}
# Availability 365 Distribution
airbnb %>%
  ggplot(aes(availability_365)) +
  geom_histogram(binwidth = 0.2, color = "black", fill = "#2e4057") +
  scale_x_continuous(name = "Availability 365",trans = pseudo_log_trans(base = 10), 
                     breaks = c(1, 10, 100, 365)) +
  ylab("Count") +
  ggtitle("Availability 365 Distribution")

# Average Listing Availability by State
airbnb %>%
  group_by(state) %>%
  summarize(mean = mean(availability_365), .groups = "drop") %>%
  ggplot(aes(x = reorder(state, mean), mean, label = round(mean, digits = 2))) +
  geom_col(color = "black", fill = "#2e4057") +
  geom_text(position = position_dodge(width = .9), hjust = -0.05, size = 3) +
  xlab("State") +
  scale_y_continuous(name = "Count", labels = comma) +
  ggtitle("Average Listing Availability by State") +
  coord_flip()
```

The longitude and latitude features give us the exact coordinates of every listing.
```{r earth-coordinates, echo = TRUE}
# Listings' earth coordinates
airbnb %>%
  ggplot(aes(x = longitude, y = latitude)) +
  geom_point(color = "black", fill = "#2e4057") +
  xlab("Longitude") +
  ylab("Latitude") +
  ggtitle("Listing Coordinates")
```

The neighborhood group, city, and state columns provide us with more broad information than the coordinates. There are variations between states, cities, and neighborhoods such as taxes, laws, and socioeconomic stature. As we can see New York, California, and Hawaii have the most listings. We can see the different average prices between top ten states, cities, and neighborhoods.
```{r neighborhood-group-distribution, echo = TRUE}
# Neighbourhood_group distribution
airbnb$neighbourhood_group <- fct_rev(fct_infreq(airbnb$neighbourhood_group))
airbnb %>%
  ggplot(aes(neighbourhood_group, label = percent(prop.table(stat(count))))) +
  geom_bar(color = "black", fill = "#2e4057") +
  geom_text(stat = "count", position = position_dodge(width = .9), 
            hjust = -0.05, size = 3) +
  xlab("Neighbourhood Group") +
  ggtitle("Neighbourhood Group Distribution") +
  coord_flip()

# Average Price by Top 10 Neighborhood Group
airbnb %>%
  group_by(neighbourhood_group) %>%
  summarize(mean = mean(price), .groups = "drop") %>%
  slice_max(mean, n = 10) %>%
  ggplot(aes(x = reorder(neighbourhood_group, mean), mean, 
             label = dollar(round(mean, digits = 2)))) +
  geom_col(color = "black", fill = "#2e4057") +
  geom_text(position = position_dodge(width = .9), hjust = -0.05, size = 3) +
  xlab("Neighborhood Group") +
  scale_y_continuous(name = "Count", labels = comma) +
  ggtitle("Average Price by Neighborhood Group") +
  coord_flip()
```


```{r cities-distribution, echo = TRUE}
# Cities Distribution
airbnb %>%
  group_by(city) %>%
  mutate(count = n()) %>%
  ggplot(aes(x = reorder(city, count), label = percent(prop.table(stat(count))))) +
  geom_bar(color = "black", fill = "#2e4057") +
  geom_text(stat = "count", position = position_dodge(width = .9), 
            hjust = -0.05, size = 3) +
  xlab("City") +
  scale_y_continuous(name = "Count", labels = comma, 
                     breaks = c(1000, 10000, 25000, 40000)) +
  ggtitle("Cities Distribution") +
  coord_flip()

# Average Price by City
airbnb %>%
  group_by(city) %>%
  summarize(mean = mean(price), .groups = "drop") %>%
  ggplot(aes(x = reorder(city, mean), mean, label = dollar(round(mean, digits = 2)))) +
  geom_col(color = "black", fill = "#2e4057") +
  geom_text(position = position_dodge(width = .9), hjust = -0.05, size = 3) +
  xlab("City") +
  scale_y_continuous(name = "Count", labels = comma) +
  ggtitle("Average Price by City") +
  coord_flip()
```


```{r state-distribution, echo = TRUE}
# State Distribution
airbnb %>%
  group_by(state) %>%
  mutate(count = n()) %>%
  ggplot(aes(x = reorder(state, count), label = percent(prop.table(stat(count))))) +
  geom_bar(color = "black", fill = "#2e4057") +
  geom_text(stat = "count", position = position_dodge(width = .9), 
            hjust = -0.05, size = 3) +
  xlab("State") +
  scale_y_continuous(name = "Count", labels = comma, 
                     breaks = c(1000, 10000, 25000, 40000, 55000)) +
  ggtitle("States Distribution") +
  coord_flip()

# Average Price by State
airbnb %>%
  group_by(state) %>%
  summarize(mean = mean(price), .groups = "drop") %>%
  ggplot(aes(x = reorder(state, mean), mean, 
             label = dollar(round(mean, digits = 2)))) +
  geom_col(color = "black", fill = "#2e4057") +
  geom_text(position = position_dodge(width = .9), hjust = -0.05, size = 3) +
  xlab("State") +
  scale_y_continuous(name = "Count", labels = comma) +
  ggtitle("Average Price by State") +
  coord_flip()
```

The name column contains a lot of information. However, we can't use it as is. We'll break this column to single words (or unigrams) and see the most common words using a wordcloud. Note that we dispose of stop words, and those that contain something other than the alphabet letters. Among the most common words are private, bedroom, home, apartment, and beach.
```{r text-analysis, echo = TRUE}
# Text analysis
words <- airbnb %>%
  unnest_tokens(word, name, drop = FALSE) %>%
  filter(!word %in% stop_words$word)

words$word <- str_replace(words$word, pattern = "[^A-Za-z]+", 
                          replacement = "") # Removing non alphabet characters

words <- words[!words$word == "",]

# Top Words
wordcloud <- words %>%
  group_by(word) %>%
  summarize(count = n(), .groups = "drop") %>%
  slice_max(count, n = 150)
wordcloud(wordcloud$word, wordcloud$count, scale = c(4, .2))
```

Now we'll add a column for every word in the top ten. This new column's values will be 0 and 1. 1 means the word is present in the name, 0 means it is missing.
```{r add-text-columns, echo = TRUE}
# Creating a new column for each word in the top 10
top_words <- words %>%
  group_by(word) %>%
  summarize(count = n(), .groups = "drop") %>%
  slice_max(count, n = 10) %>%
  pull(word)

airbnb <- airbnb %>% mutate(name = tolower(name))

for (word in top_words){
  regex_word <- word
  col_name <- paste("word", word, sep="_")
  airbnb <- airbnb %>% 
    mutate(!!col_name := ifelse(grepl(regex_word, name, fixed = TRUE), 1, 0))
}
```

Another powerful analysis tool for text is sentiment analysis. We'll use the afinn dictionary consisting of 2,477 coded words. Afinn gives every word in the dictionary a sentiment value between -5 for the most negative and 5 for the most positive sentiment. Let's see the sentiment value of the top ten used words.
```{r sentiment-analysis, echo = TRUE}
# Sentiment Analysis
afinn <- get_sentiments("afinn")
words_sentiment <- words %>% 
  inner_join(afinn, by = "word") %>%
  rename(sentiment = value)

# Top 10 Words
words_sentiment %>%
  group_by(word, sentiment) %>%
  summarize(count = n(), .groups = "drop") %>%
  slice_max(count, n = 10)
```

We'll combine the total sentiment value of the words in every name and create a column for that.
```{r add-sentiment-column, echo = TRUE}
# Creating the sentiment column
words <- words %>% 
  left_join(afinn, by = "word") %>%
  rename(sentiment = value)

id_sentiment <- words %>% 
  group_by(id) %>% 
  summarize(sentiment = sum(sentiment, na.rm = TRUE), .groups = "drop")

airbnb <- airbnb %>%
  inner_join(id_sentiment, by = "id")
```

Looking at the correlation plot, there are no notable correlations between columns. The number of reviews and reviews per month are correlated to some extend but we'll keep both of them from reasons explained above. There is also correlation between longitude and latitude, but that is expected as the listings are clustered only in the U.S. and we know they complete each other.
```{r correlation-plot, echo = TRUE}
# The correlation plot helps us find columns that explain each other.
airbnb %>%
  select_if(is.numeric) %>%
  cor %>%
  corrplot(type = "upper", order = "hclust", tl.col = "black", tl.cex = 0.6)
```

Let's convert our categorical columns to dummy columns.
```{r add-dummy-columns, echo = TRUE}
# Adding dummy columns for categorical columns.
airbnb <- dummy_cols(airbnb, 
                     select_columns = c("neighbourhood_group", "room_type", "state", "city"))
airbnb <- airbnb %>%
  select(!(any_of(c("id", "name", "host_id", "host_name", "neighbourhood",
                    "neighbourhood_group", "room_type", "state", "region", "city"))))
names(airbnb) <- str_replace_all(names(airbnb), c(" " = "_", "/" = "_"))

# Turn character columns to factor columns
airbnb <- airbnb %>% mutate_if(is.character, as.factor)
```

And normalize our data.
```{r normalization, echo = TRUE}
# The correlation plot helps us find columns that explain each other.
# Normalize
airbnb <- BBmisc::normalize(airbnb, method = "range", range = c(0, 1))
```

We'll remove columns with near zero variance set to the default 95%. Columns containing more than 95% of the same value will be removed since they don't add a lot more information.
```{r nzv, echo = TRUE}
# Removing column with near zero variance since they are not very useful for 
# prediction and can prolong running time.
nzv <- nearZeroVar(airbnb, )
if (length(nzv) > 0) {
  airbnb <- airbnb[, -nzv]
}
```

Now the data is ready for modeling. We'll create a training set and testing set with a 9 to 1 ratio, and a small training set of 10,000 rows to tune our models.
```{r data-split, echo = TRUE, warning = FALSE}
# Taking 10% as test test.
set.seed(1, sample.kind="Rounding")
test_indices <- createDataPartition(y = airbnb$price, times = 1, p = 0.1, 
                                    list = FALSE)
train <- airbnb[-test_indices,]
test <- airbnb[test_indices,]
train_small_subset <- train %>% sample_n(10000)
```

## Modeling

### Average model
This model takes the average price and predicts it for every new listing. This is our base naive model.
```{r mean-model, echo = TRUE}
# Mean calculation
mu <- mean(train$price)
mu

# Testing results
mu_rmse <- RMSE(test$price, mu)
mu_rmse
```


```{r rmse-table-init, echo = TRUE}
# Initializing a RMSE table to save the results
rmse_results <- tibble(method = "Average Price Model", RMSE = mu_rmse)
rmse_results %>% knitr::kable()
```

### Linear Regression model
We'll train a linear regression model with 10 fold cross validation.
```{r lm-model, echo = TRUE, warning = FALSE}
# Linear Regression
control <- trainControl(method = "cv", number = 10)
fit_lm <- train(price ~ ., method = "lm", data = train, trControl = control)
predictions_lm <- predict(fit_lm, test)
lm_rmse <- RMSE(test$price, predictions_lm)
lm_rmse
```

There is small improvement compared to the naive model. Let's see how the model predicts against the true values.
```{r lm-predictions-true, echo = TRUE}
actual_vs_pred <- data.frame(x = test$price, y = predictions_lm)
ggplot(actual_vs_pred ,aes(x, y)) +
  geom_point() +
  geom_smooth(formula = "y ~ x", method='lm') +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Linear Regression") +
  xlab("Actual Prices") +
  ylab("Predicted Prices")
```


```{r add-lm-rmse, echo = TRUE}
rmse_results <- rmse_results %>%
  add_row(method = "Linear Regression", RMSE = lm_rmse)
rmse_results %>% knitr::kable()
```

### KNN model
For the KNN model, we first train a model on the small subset to tune the K parameter. The train function will use 10 fold cross validation. The K parameter determines how many neighbors should the algorithm look for to decide the label of the current row. The following plot shows the different values for K and their corresponding RMSE.
```{r train-knn, echo = TRUE}
## KNN ##
tune <- expand.grid(k = c(9, 15, 21, 29, 41, 55, 71))
train_knn <- train(price ~ ., method = "knn", data = train_small_subset, 
                   trControl = control, tuneGrid = tune)
ggplot(train_knn)
train_knn$bestTune
```

Now we train the real knn model using the K value we found earlier.
```{r fit-knn, echo = TRUE}
fit_knn <- knnreg(price ~ ., data = train, k = train_knn$bestTune)
predictions_knn <- predict(fit_knn, test)
knn_rmse <- RMSE(test$price, predictions_knn)
knn_rmse
```

Again, we see a small improvement from the previous model.
```{r knn-predictions-true, echo = TRUE}
actual_vs_pred <- data.frame(x = test$price, y = predictions_knn)
ggplot(actual_vs_pred ,aes(x, y)) +
  geom_point() +
  geom_smooth(formula = "y ~ x", method='glm') +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Knn") +
  xlab("Actual Prices") +
  ylab("Predicted Prices")
```


```{r add-knn-rmse, echo = TRUE}
rmse_results <- rmse_results %>%
  add_row(method = "Knn", RMSE = knn_rmse)
rmse_results %>% knitr::kable()
```

### Regression Tree model
For the regression tree model we will also start by training a small subset with 10 fold cross validation to tune the cp parameter. The cp parameter, stands for complexity parameter, helps determine the right size of the tree. Let's plot the different values of the cp parameter and their corresponding rmse values.
```{r train-rt, echo = TRUE}
## Regression Tree ##
tune <- expand.grid(cp = seq(0, 0.0002, len = 7))
train_rt <- train(price ~ ., method = "rpart", data = train, trControl = control, 
                  tuneGrid = tune)
train_rt$bestTune
ggplot(train_rt)
```

Using the best cp value, we'll train the regression tree model on all the data.
```{r fit-rt, echo = TRUE}
fit_rt <- rpart(price ~ ., data = train, 
                control = rpart.control(cp = train_rt$bestTune))
predictions_rt <- predict(fit_rt, test)
rt_rmse <- RMSE(test$price, predictions_rt)
rt_rmse
```

The rmse has, again, been improved slightly. Let's see how the predictions compare to the real values.
```{r rt-predictions-true, echo = TRUE}
actual_vs_pred <- data.frame(x = test$price, y = predictions_rt)
ggplot(actual_vs_pred ,aes(x, y)) +
  geom_point() +
  geom_smooth(formula = "y ~ x", method='glm') +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Regression Tree") +
  xlab("Actual Prices") +
  ylab("Predicted Prices")
```


```{r add-rt-rmse, echo = TRUE}
rmse_results <- rmse_results %>%
  add_row(method = "Regression Tree", RMSE = rt_rmse)
rmse_results %>% knitr::kable()
```

### Random Forest model
For the random forest model, we train a small subset to tune the mtry parameter. The mtry parameter represents the number of columns to consider when looking for the column to split by. We'll use an estimation that is third of the number of columns and try a few integer above and below this number. We also produce a plot of the mtry parameters and their corresponding rmse values. In order to save some running time, we'll limit the number of trees to 100.
```{r train-rf, echo = TRUE}
## Random Forest ##
mtry <- round(ncol(train) / 3)
tune <- expand.grid(mtry = seq(mtry - 3, mtry + 3, len = 7))
train_rf <- train(price ~ ., method = "rf", data = train_small_subset, 
                  ntree = 100, trControl = control, tuneGrid = tune)
ggplot(train_rf)
train_rf$bestTune
```

Now using the mtry parameter found, we train the random forest model on the whole data. This time, we'll limit the number of trees to 150 to save some running time.
```{r fit-rf, echo = TRUE}
fit_rf <- randomForest(price ~ ., data = train, 
                       minNode = fit_rf$bestTune$mtry, ntree = 150)
predictions_rf <- predict(fit_rf, test)
rf_rmse <- RMSE(test$price, predictions_rf)
rf_rmse
```

There is more satisfying improvement than previous model in rmse terms. Let's see the comparison between the predictions and true values.
```{r rf-predictions-true, echo = TRUE}
actual_vs_pred <- data.frame(x = test$price, y = predictions_rf)
ggplot(actual_vs_pred ,aes(x, y)) +
  geom_point() +
  geom_smooth(formula = "y ~ x", method='glm') +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Random Forest") +
  xlab("Actual Prices") +
  ylab("Predicted Prices")
```


```{r add-rf-rmse, echo = TRUE}
rmse_results <- rmse_results %>%
  add_row(method = "Random Forest", RMSE = rf_rmse)
rmse_results %>% knitr::kable()
```

### Average Ensemble model
We'll try to average all predictions of previous models (without the basic naive one) the get a new set of predictions. This is the average ensemble.
```{r avg-ensemble, echo = TRUE}
# Average Ensemble
predictions_ensemble <- (predictions_lm + predictions_knn +
      predictions_rt + predictions_rf) / 4
ensemble_rmse <- RMSE(test$price, predictions_ensemble)
ensemble_rmse
```

Unfortunately it is not better than the random forest model, but it is the second best model.
```{r avg-ensemble-predictions-true, echo = TRUE}
actual_vs_pred <- data.frame(x = test$price, y = predictions_ensemble)
ggplot(actual_vs_pred ,aes(x, y)) +
  geom_point() +
  geom_smooth(formula = "y ~ x", method='glm') +
  geom_abline(slope = 1, intercept = 0, color = "red") +
  ggtitle("Average Ensemble") +
  xlab("Actual Prices") +
  ylab("Predicted Prices")
```


```{r add-avg-ensemble-rmse, echo = TRUE}
rmse_results <- rmse_results %>%
  add_row(method = "Average Ensemble", RMSE = ensemble_rmse)
rmse_results %>% knitr::kable()
```

When we look at the data, we can see that most of it is at the 10 to 350 dollar price range, about 0.85 of the data. Let's remove all rows with price outside that range and try to see how the models perform on these values.
```{r test-low-prices, echo = TRUE}
# A look at the models' predictions using rows with price less than $350.
low_prices_test <- test %>%
  filter(price < 0.35)
```

```{r lm-low-prices, echo = TRUE, warning = FALSE}
# Linear Regression
predictions_lm_low_prices <- predict(fit_lm, low_prices_test)
lm_rmse_low_prices <- RMSE(low_prices_test$price, predictions_lm_low_prices)

rmse_results <- rmse_results %>%
  add_row(method = "Linear Regression, low prices", RMSE = lm_rmse_low_prices)
rmse_results %>% knitr::kable()
```

```{r knn-low-prices, echo = TRUE}
# KNN
predictions_knn_low_prices <- predict(fit_knn, low_prices_test)
knn_rmse_low_prices <- RMSE(low_prices_test$price, predictions_knn_low_prices)

rmse_results <- rmse_results %>%
  add_row(method = "Knn, low prices", RMSE = knn_rmse_low_prices)
rmse_results %>% knitr::kable()
```

```{r rt-low-prices, echo = TRUE}
# Regression Tree
predictions_rt_low_prices <- predict(fit_rt, low_prices_test)
rt_rmse_low_prices <- RMSE(low_prices_test$price, predictions_rt_low_prices)

rmse_results <- rmse_results %>%
  add_row(method = "Regression Tree, low prices", RMSE = rt_rmse_low_prices)
rmse_results %>% knitr::kable()
```

```{r rf-low-prices, echo = TRUE}
# Random Forest
predictions_rf_low_prices <- predict(fit_rf, low_prices_test)
rf_rmse_low_prices <- RMSE(low_prices_test$price, predictions_rf_low_prices)

rmse_results <- rmse_results %>%
  add_row(method = "Random Forest, low prices", RMSE = rf_rmse_low_prices)
rmse_results %>% knitr::kable()
```

```{r avg-ensemble-low-prices, echo = TRUE}
# Average Ensemble
predictions_ensemble_low_prices <- (predictions_lm_low_prices + 
                                      predictions_knn_low_prices +
                                      predictions_rt_low_prices + 
                                      predictions_rf_low_prices) / 4
ensemble_rmse_low_prices <- RMSE(low_prices_test$price, predictions_ensemble_low_prices)

rmse_results <- rmse_results %>%
  add_row(method = "Average Ensemble, low prices", RMSE = ensemble_rmse_low_prices)
rmse_results %>% knitr::kable()
```

There is a significant improvement! This implies that most of the error came from the high price values, where the data is pretty scarce. 

# Results
```{r results, echo = FALSE}
rmse_results %>% knitr::kable()
```
The best model overall was the random forest model. He had the best results on the whole dataset, and was best together with the ensemble at the low prices. 

# Discussion
The random forest model used a mtry parameter of 11. Out of the 33 columns available to the train function, evaluating 11 is enough to get a good balance between running time and rmse results. Also, the model generats only 150 trees, which by my experiments is enough to get very close to the minimum rmse and save a lot of running time. Note that the random forest model takes about 2-3 hours to run.

# Conclusion
We could predict listings prices with an error of 10.9% on the whole dataset, and 6.91% on prices below $350. In order to improve the model, I would say more features are needed, such as the reviews themselves or number of stars that can help differ strong listings from weak ones. Also, in order to predict more accurately on the higher prices, a lot more data is needed. The data available at these values is not enough to get the general trend of the price.

\pagebreak

# Appendix
## Environment
Operating System:
```{r environment, echo=FALSE}
version
```
